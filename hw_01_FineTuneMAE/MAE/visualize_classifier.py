#!/usr/bin/env python3
"""
åˆ†é¡å™¨çµæœå¯è¦–åŒ–è…³æœ¬
ç”¨æ–¼å¯è¦–åŒ–å·²è¨“ç·´çš„åˆ†é¡å™¨åœ¨æ¸¬è©¦é›†ä¸Šçš„é æ¸¬çµæœ,ç„¡éœ€é‡æ–°è¨“ç·´
"""
import os
import argparse
import torch
import torchvision
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from torchvision.transforms import ToTensor, Compose, Normalize
from tqdm import tqdm

from model import ViT_Classifier
from utils import setup_seed

# CIFAR-10 é¡åˆ¥åç¨±
CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
                   'dog', 'frog', 'horse', 'ship', 'truck']

def visualize_predictions(model, dataset, device, num_samples=16, output_path='classifier_visualization.png', scale_factor=4):
    """
    å¯è¦–åŒ–åˆ†é¡å™¨çš„é æ¸¬çµæœ
    
    Args:
        model: è¨“ç·´å¥½çš„åˆ†é¡å™¨æ¨¡å‹
        dataset: æ¸¬è©¦æ•¸æ“šé›†
        device: è¨ˆç®—è¨­å‚™
        num_samples: è¦å¯è¦–åŒ–çš„æ¨£æœ¬æ•¸é‡ (å»ºè­° 16, 25, 36 ç­‰å®Œå…¨å¹³æ–¹æ•¸)
        output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
        scale_factor: åœ–ç‰‡æ”¾å¤§å€æ•¸
    """
    model.eval()
    
    # è¨ˆç®—ç¶²æ ¼å¤§å° (ç›¡é‡æ¥è¿‘æ­£æ–¹å½¢)
    grid_size = int(np.sqrt(num_samples))
    actual_samples = grid_size * grid_size
    
    print(f'æ­£åœ¨å¯è¦–åŒ– {actual_samples} å€‹æ¨£æœ¬ ({grid_size}x{grid_size} ç¶²æ ¼)...')
    
    with torch.no_grad():
        # å–å‡ºæŒ‡å®šæ•¸é‡çš„åœ–ç‰‡
        images = torch.stack([dataset[i][0] for i in range(actual_samples)])
        labels = torch.tensor([dataset[i][1] for i in range(actual_samples)])
        
        images = images.to(device)
        labels = labels.to(device)
        
        # é æ¸¬
        logits = model(images)
        predictions = logits.argmax(dim=-1)
        
        # è¨ˆç®—æº–ç¢ºç‡
        accuracy = (predictions == labels).float().mean().item()
        print(f'é€™ {actual_samples} å€‹æ¨£æœ¬çš„æº–ç¢ºç‡: {accuracy*100:.2f}%')
        
        # å°‡åœ–ç‰‡æ”¾å¤§ä»¥ä¾¿æŸ¥çœ‹
        images_large = F.interpolate(images.cpu(), scale_factor=scale_factor, mode='nearest')
        
        # å°‡åœ–ç‰‡å¾ [-1, 1] è½‰æ›åˆ° [0, 1]
        images_large = (images_large + 1) / 2
        
        # å‰µå»ºå¯è¦–åŒ–
        fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*2, grid_size*2))
        fig.suptitle(f'Classifier Predictions (Accuracy: {accuracy*100:.2f}%)', fontsize=16, y=0.995)
        
        for idx in range(actual_samples):
            row = idx // grid_size
            col = idx % grid_size
            ax = axes[row, col] if grid_size > 1 else axes
            
            # é¡¯ç¤ºåœ–ç‰‡
            img = images_large[idx].permute(1, 2, 0).numpy()
            img = np.clip(img, 0, 1)  # ç¢ºä¿åœ¨æœ‰æ•ˆç¯„åœå…§
            ax.imshow(img)
            
            # è¨­ç½®æ¨™é¡Œ (çœŸå¯¦æ¨™ç±¤ vs é æ¸¬æ¨™ç±¤)
            true_label = CIFAR10_CLASSES[labels[idx].item()]
            pred_label = CIFAR10_CLASSES[predictions[idx].item()]
            is_correct = predictions[idx] == labels[idx]
            
            title_color = 'green' if is_correct else 'red'
            title = f'GT: {true_label}\nPred: {pred_label}'
            ax.set_title(title, fontsize=10, color=title_color)
            ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f'âœ… å¯è¦–åŒ–çµæœå·²ä¿å­˜åˆ°: {output_path}')
        plt.close()

def evaluate_model(model, dataloader, device):
    """
    åœ¨æ•´å€‹æ•¸æ“šé›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model: è¨“ç·´å¥½çš„åˆ†é¡å™¨æ¨¡å‹
        dataloader: æ•¸æ“šåŠ è¼‰å™¨
        device: è¨ˆç®—è¨­å‚™
    
    Returns:
        accuracy: æº–ç¢ºç‡
        per_class_acc: æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
    """
    model.eval()
    
    all_predictions = []
    all_labels = []
    
    print('æ­£åœ¨è©•ä¼°æ¨¡å‹...')
    with torch.no_grad():
        for img, label in tqdm(dataloader):
            img = img.to(device)
            label = label.to(device)
            
            logits = model(img)
            predictions = logits.argmax(dim=-1)
            
            all_predictions.append(predictions.cpu())
            all_labels.append(label.cpu())
    
    all_predictions = torch.cat(all_predictions)
    all_labels = torch.cat(all_labels)
    
    # ç¸½é«”æº–ç¢ºç‡
    accuracy = (all_predictions == all_labels).float().mean().item()
    
    # æ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡
    per_class_acc = {}
    for cls_idx, cls_name in enumerate(CIFAR10_CLASSES):
        mask = all_labels == cls_idx
        if mask.sum() > 0:
            cls_acc = (all_predictions[mask] == all_labels[mask]).float().mean().item()
            per_class_acc[cls_name] = cls_acc
    
    return accuracy, per_class_acc

def create_confusion_matrix_plot(model, dataloader, device, output_path='confusion_matrix.png'):
    """
    å‰µå»ºä¸¦ä¿å­˜æ··æ·†çŸ©é™£åœ–
    
    Args:
        model: è¨“ç·´å¥½çš„åˆ†é¡å™¨æ¨¡å‹
        dataloader: æ•¸æ“šåŠ è¼‰å™¨
        device: è¨ˆç®—è¨­å‚™
        output_path: è¼¸å‡ºåœ–ç‰‡è·¯å¾‘
    """
    model.eval()
    
    # åˆå§‹åŒ–æ··æ·†çŸ©é™£
    num_classes = len(CIFAR10_CLASSES)
    confusion_matrix = torch.zeros(num_classes, num_classes, dtype=torch.int64)
    
    print('æ­£åœ¨ç”Ÿæˆæ··æ·†çŸ©é™£...')
    with torch.no_grad():
        for img, label in tqdm(dataloader):
            img = img.to(device)
            label = label.to(device)
            
            logits = model(img)
            predictions = logits.argmax(dim=-1)
            
            # æ›´æ–°æ··æ·†çŸ©é™£
            for true_label, pred_label in zip(label.cpu(), predictions.cpu()):
                confusion_matrix[true_label, pred_label] += 1
    
    # ç¹ªè£½æ··æ·†çŸ©é™£
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # å°‡æ··æ·†çŸ©é™£è½‰æ›ç‚ºç™¾åˆ†æ¯”
    confusion_matrix_pct = confusion_matrix.float() / confusion_matrix.sum(dim=1, keepdim=True) * 100
    
    im = ax.imshow(confusion_matrix_pct.numpy(), cmap='Blues', aspect='auto')
    
    # è¨­ç½®åˆ»åº¦å’Œæ¨™ç±¤
    ax.set_xticks(np.arange(num_classes))
    ax.set_yticks(np.arange(num_classes))
    ax.set_xticklabels(CIFAR10_CLASSES, rotation=45, ha='right')
    ax.set_yticklabels(CIFAR10_CLASSES)
    
    # åœ¨æ¯å€‹æ ¼å­ä¸­é¡¯ç¤ºæ•¸å€¼
    for i in range(num_classes):
        for j in range(num_classes):
            text = ax.text(j, i, f'{confusion_matrix[i, j]}\n({confusion_matrix_pct[i, j]:.1f}%)',
                          ha="center", va="center", color="black" if confusion_matrix_pct[i, j] < 50 else "white",
                          fontsize=8)
    
    ax.set_title('Confusion Matrix', fontsize=16, pad=20)
    ax.set_xlabel('Predicted Label', fontsize=12)
    ax.set_ylabel('True Label', fontsize=12)
    
    # æ·»åŠ é¡è‰²æ¢
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('Percentage (%)', rotation=270, labelpad=20)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f'âœ… æ··æ·†çŸ©é™£å·²ä¿å­˜åˆ°: {output_path}')
    plt.close()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='å¯è¦–åŒ–åˆ†é¡å™¨é æ¸¬çµæœ')
    parser.add_argument('--model_path', type=str, required=True, help='è¨“ç·´å¥½çš„åˆ†é¡å™¨æ¨¡å‹è·¯å¾‘')
    parser.add_argument('--output_dir', type=str, default='pic', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--num_samples', type=int, default=16, help='å¯è¦–åŒ–çš„æ¨£æœ¬æ•¸é‡ (å»ºè­°ä½¿ç”¨å®Œå…¨å¹³æ–¹æ•¸)')
    parser.add_argument('--batch_size', type=int, default=512, help='è©•ä¼°æ™‚çš„ batch size')
    parser.add_argument('--seed', type=int, default=42, help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--scale_factor', type=int, default=4, help='åœ–ç‰‡æ”¾å¤§å€æ•¸')
    parser.add_argument('--skip_confusion_matrix', action='store_true', help='è·³éæ··æ·†çŸ©é™£ç”Ÿæˆ')
    
    args = parser.parse_args()
    
    setup_seed(args.seed)
    
    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æå–æ¨¡å‹åç¨±ä½œç‚ºæ–‡ä»¶å‰ç¶´
    model_name = os.path.splitext(os.path.basename(args.model_path))[0]
    
    # è¼‰å…¥æ•¸æ“šé›†
    print('è¼‰å…¥ CIFAR-10 æ¸¬è©¦é›†...')
    test_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, 
                                                 transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))
    test_dataloader = torch.utils.data.DataLoader(test_dataset, args.batch_size, 
                                                   shuffle=False, num_workers=4)
    
    # è¼‰å…¥æ¨¡å‹
    print(f'è¼‰å…¥æ¨¡å‹: {args.model_path}')
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = torch.load(args.model_path, map_location=device)
    model.to(device)
    model.eval()
    
    # 1. è©•ä¼°æ•´é«”æ€§èƒ½
    print('\n' + '='*70)
    print('è©•ä¼°æ¨¡å‹æ€§èƒ½')
    print('='*70)
    accuracy, per_class_acc = evaluate_model(model, test_dataloader, device)
    
    print(f'\nç¸½é«”æº–ç¢ºç‡: {accuracy*100:.2f}%')
    print('\næ¯å€‹é¡åˆ¥çš„æº–ç¢ºç‡:')
    print('-'*40)
    for cls_name, cls_acc in per_class_acc.items():
        print(f'{cls_name:12s}: {cls_acc*100:.2f}%')
    
    # ä¿å­˜è©•ä¼°çµæœåˆ°æ–‡æœ¬æ–‡ä»¶
    results_path = os.path.join(args.output_dir, f'{model_name}_results.txt')
    with open(results_path, 'w') as f:
        f.write(f'Model: {args.model_path}\n')
        f.write(f'Overall Accuracy: {accuracy*100:.2f}%\n\n')
        f.write('Per-class Accuracy:\n')
        f.write('-'*40 + '\n')
        for cls_name, cls_acc in per_class_acc.items():
            f.write(f'{cls_name:12s}: {cls_acc*100:.2f}%\n')
    print(f'\nâœ… è©•ä¼°çµæœå·²ä¿å­˜åˆ°: {results_path}')
    
    # 2. å¯è¦–åŒ–é æ¸¬çµæœ
    print('\n' + '='*70)
    print('ç”Ÿæˆé æ¸¬å¯è¦–åŒ–')
    print('='*70)
    vis_path = os.path.join(args.output_dir, f'{model_name}_predictions.png')
    visualize_predictions(model, test_dataset, device, 
                         num_samples=args.num_samples, 
                         output_path=vis_path,
                         scale_factor=args.scale_factor)
    
    # 3. ç”Ÿæˆæ··æ·†çŸ©é™£
    if not args.skip_confusion_matrix:
        print('\n' + '='*70)
        print('ç”Ÿæˆæ··æ·†çŸ©é™£')
        print('='*70)
        cm_path = os.path.join(args.output_dir, f'{model_name}_confusion_matrix.png')
        create_confusion_matrix_plot(model, test_dataloader, device, output_path=cm_path)
    
    print('\n' + '='*70)
    print('ğŸ‰ æ‰€æœ‰å¯è¦–åŒ–å®Œæˆ!')
    print('='*70)
    print(f'\nç”Ÿæˆçš„æ–‡ä»¶:')
    print(f'  - é æ¸¬å¯è¦–åŒ–: {vis_path}')
    if not args.skip_confusion_matrix:
        print(f'  - æ··æ·†çŸ©é™£: {cm_path}')
    print(f'  - è©•ä¼°çµæœ: {results_path}')
    print('='*70)
