import os
import argparse
import math
import torch
import torchvision
from torch.utils.tensorboard import SummaryWriter
from torchvision.transforms import ToTensor, Compose, Normalize
from tqdm import tqdm
import torch.nn.functional as F

from model import *
from utils import setup_seed

def calculate_psnr(img1, img2, max_val=1.0):
    """Calculate PSNR between two images"""
    mse = torch.mean((img1 - img2) ** 2)
    if mse == 0:
        return float('inf')
    psnr = 20 * torch.log10(max_val / torch.sqrt(mse))
    return psnr.item()

def calculate_ssim(img1, img2, window_size=11, size_average=True):
    """Calculate SSIM between two images (simplified version)"""
    C1 = 0.01 ** 2
    C2 = 0.03 ** 2
    
    mu1 = F.avg_pool2d(img1, window_size, 1, padding=window_size//2)
    mu2 = F.avg_pool2d(img2, window_size, 1, padding=window_size//2)
    
    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2
    
    sigma1_sq = F.avg_pool2d(img1 * img1, window_size, 1, padding=window_size//2) - mu1_sq
    sigma2_sq = F.avg_pool2d(img2 * img2, window_size, 1, padding=window_size//2) - mu2_sq
    sigma12 = F.avg_pool2d(img1 * img2, window_size, 1, padding=window_size//2) - mu1_mu2
    
    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
    
    if size_average:
        return ssim_map.mean().item()
    else:
        return ssim_map.mean(1).mean(1).mean(1)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--batch_size', type=int, default=4096) # æ¢¯åº¦ç´¯ç©å¾Œçš„ç­‰æ•ˆ batch size
    parser.add_argument('--max_device_batch_size', type=int, default=512) # æ¯å€‹ GPU çš„æœ€å¤§ batch size
    parser.add_argument('--base_learning_rate', type=float, default=1.5e-4)
    parser.add_argument('--weight_decay', type=float, default=0.05)
    parser.add_argument('--mask_ratio', type=float, default=0.75)
    # parser.add_argument('--total_epoch', type=int, default=2000)
    # parser.add_argument('--warmup_epoch', type=int, default=200)
    parser.add_argument('--total_epoch', type=int, default=200)
    parser.add_argument('--warmup_epoch', type=int, default=20)
    parser.add_argument('--model_path', type=str, default='vit-t-mae-final.pt')
    
    # æ¶ˆèžå¯¦é©—åƒæ•¸: ç”¨æ–¼æ¸¬è©¦ä¸åŒçš„è§£ç¢¼å™¨é…ç½®
    parser.add_argument('--decoder_layer', type=int, default=4, help='è§£ç¢¼å™¨å±¤æ•¸ (å¯¦é©— a: 1,2,4,8,12)')
    parser.add_argument('--decoder_dim', type=int, default=None, help='è§£ç¢¼å™¨ç¶­åº¦ (å¯¦é©— b: 128,256,512,768,1024), è‹¥ç‚º None å‰‡èˆ‡ encoder ç›¸åŒ')
    parser.add_argument('--log_dir', type=str, default=None, help='TensorBoard æ—¥èªŒç›®éŒ„ (è‹¥ç‚º None å‰‡è‡ªå‹•ç”Ÿæˆ)')

    args = parser.parse_args()

    setup_seed(args.seed)

    batch_size = args.batch_size
    load_batch_size = min(args.max_device_batch_size, batch_size)

    assert batch_size % load_batch_size == 0
    steps_per_update = batch_size // load_batch_size

    # transform: totensor è³‡æ–™æœƒå¾ž [0, 255] è®Šæˆ [0, 1], normalize å¾Œæœƒè®Šæˆ [-1, 1]
    train_dataset = torchvision.datasets.CIFAR10('data', train=True, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))
    val_dataset = torchvision.datasets.CIFAR10('data', train=False, download=True, transform=Compose([ToTensor(), Normalize(0.5, 0.5)]))
    dataloader = torch.utils.data.DataLoader(train_dataset, load_batch_size, shuffle=True, num_workers=4)
    
    # æ ¹æ“šå¯¦é©—é…ç½®ç”Ÿæˆç¨ç«‹çš„æ—¥èªŒç›®éŒ„,é¿å…ä¸åŒå¯¦é©—çš„æ—¥èªŒäº’ç›¸è¦†è“‹
    if args.log_dir is None:
        # å¦‚æžœä½¿ç”¨é»˜èªåƒæ•¸,ä½¿ç”¨é»˜èªæ—¥èªŒç›®éŒ„
        if args.decoder_layer == 4 and args.decoder_dim is None:
            log_dir = os.path.join('logs', 'cifar10', 'mae-pretrain')
        else:
            # æ¶ˆèžå¯¦é©—:æ ¹æ“šé…ç½®ç”Ÿæˆå”¯ä¸€çš„æ—¥èªŒç›®éŒ„åç¨±
            decoder_dim_str = str(args.decoder_dim) if args.decoder_dim is not None else '192'
            log_dir = os.path.join('logs', 'cifar10', 'ablation', f'mae_layer{args.decoder_layer}_dim{decoder_dim_str}')
    else:
        log_dir = args.log_dir
    
    writer = SummaryWriter(log_dir)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # å‰µå»º MAE æ¨¡åž‹,æ”¯æ´è‡ªå®šç¾©è§£ç¢¼å™¨é…ç½®
    # encoder ç¶­åº¦å›ºå®šç‚º 192, encoder å±¤æ•¸å›ºå®šç‚º 12
    # å¯èª¿æ•´çš„æ˜¯: decoder_layer (è§£ç¢¼å™¨å±¤æ•¸) å’Œ decoder_emb_dim (è§£ç¢¼å™¨ç¶­åº¦)
    model = MAE_ViT(
        image_size=32,
        patch_size=2,
        emb_dim=192,           # encoder ç¶­åº¦ (å›ºå®š)
        encoder_layer=12,      # encoder å±¤æ•¸ (å›ºå®š)
        encoder_head=3,
        decoder_layer=args.decoder_layer,      # è§£ç¢¼å™¨å±¤æ•¸ (å¯èª¿æ•´,ç”¨æ–¼å¯¦é©— a)
        decoder_head=3,
        mask_ratio=args.mask_ratio,
        decoder_emb_dim=args.decoder_dim       # è§£ç¢¼å™¨ç¶­åº¦ (å¯èª¿æ•´,ç”¨æ–¼å¯¦é©— b)
    ).to(device)
    optim = torch.optim.AdamW(model.parameters(), lr=args.base_learning_rate * args.batch_size / 256, betas=(0.9, 0.95), weight_decay=args.weight_decay)
    lr_func = lambda epoch: min((epoch + 1) / (args.warmup_epoch + 1e-8), 0.5 * (math.cos(epoch / args.total_epoch * math.pi) + 1))
    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_func, verbose=True)

    step_count = 0
    optim.zero_grad()
    best_ssim = 0.0  # è¨˜éŒ„æœ€ä½³ SSIM å€¼
    
    for e in range(args.total_epoch):
        model.train()
        losses = []
        for img, label in tqdm(iter(dataloader)):
            step_count += 1
            img = img.to(device)
            predicted_img, mask = model(img)
            loss = torch.mean((predicted_img - img) ** 2 * mask) / args.mask_ratio
            loss.backward()
            if step_count % steps_per_update == 0: # æ¢¯åº¦ç´¯ç© æ¯ 8 epoch æ›´æ–°ä¸€æ¬¡
                optim.step()
                optim.zero_grad()
            losses.append(loss.item())
        lr_scheduler.step()
        avg_loss = sum(losses) / len(losses)
        writer.add_scalar('tr/lr', optim.param_groups[0]['lr'], global_step=e)
        writer.add_scalar('tr/MSE_loss', avg_loss, global_step=e)
        print(f'In epoch {e}, average traning loss is {avg_loss}.')

        ''' visualize the first 16 predicted images on val dataset and compute metrics'''
        model.eval()
        with torch.no_grad():
            val_img = torch.stack([val_dataset[i][0] for i in range(16)]) # for å¾žé©—è­‰é›†å–å‰16å¼µåœ– # stack å°‡åœ–ç‰‡æ‹¼æˆä¸€å€‹ batch [16, c, h, w]
            val_img = val_img.to(device)
            predicted_val_img, mask = model(val_img) # predicted_val_img ä¿®å¾©çš„å®Œæ•´åœ–ç‰‡ # mask åœ¨ enc ç•¶ä¸­è¢«éš¨æ©Ÿé®è”½çš„ä½ç½®

            # # åŽŸå§‹å¯«æ³•
            # predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)
            # img = torch.cat([val_img * (1 - mask), predicted_val_img, val_img], dim=0)
            
            ############# ä¿®æ”¹æ®µè½ å¢žåŠ è©•ä¼°æŒ‡æ¨™ ä¿å­˜ best model ä»¥åŠç·šæ€§æ’å€¼æ”¾å¤§å¯è¦–åŒ–çµæžœ4x4å€ (åŽŸä¸€å¼µåœ–åªæœ‰32x32çœ‹åˆ°çœ¼ç›è„«çª—) #############
            # è¨ˆç®—è©•ä¼°æŒ‡æ¨™ï¼ˆåªåœ¨è¢«é®ç½©çš„å€åŸŸï¼‰
            masked_region = predicted_val_img * mask # pred
            original_masked = val_img * mask         # gt
            
            # å°‡åœ–ç‰‡ç¯„åœå¾ž [-1, 1] è½‰æ›åˆ° [0, 1] ä»¥ä¾¿è¨ˆç®—æŒ‡æ¨™ (å¾ž transform çš„ Normalize(0.5, 0.5) å¯çŸ¥)
            masked_region_norm = (masked_region + 1) / 2
            original_masked_norm = (original_masked + 1) / 2
            
            # MSE (on masked region only)
            mse = torch.mean((masked_region - original_masked) ** 2).item()
            
            # MAE (on masked region only)
            mae = torch.mean(torch.abs(masked_region - original_masked)).item()
            
            # PSNR (on masked region only)
            # Peak Signal-to-Noise Ratio: å³°å€¼ä¿¡å™ªæ¯”ï¼Œé€šå¸¸åœ¨ 20-40 dB ä¹‹é–“ï¼Œè¶Šé«˜è¶Šå¥½
            psnr = calculate_psnr(masked_region_norm, original_masked_norm, max_val=1.0)
            
            # SSIM (on full reconstructed image) 
            # Structural Similarity Index: çµæ§‹ç›¸ä¼¼æ€§ï¼Œç¯„åœ 0-1ï¼Œè¶ŠæŽ¥è¿‘ 1 è¡¨ç¤ºé‡å»ºå“è³ªè¶Šå¥½
            predicted_val_img_full = predicted_val_img * mask + val_img * (1 - mask) # predçš„maskå€åŸŸ + gtçš„éžmaskå€åŸŸ
            predicted_val_img_full_norm = (predicted_val_img_full + 1) / 2           # å¾ž [-1, 1] normåˆ° [0, 1]
            val_img_norm = (val_img + 1) / 2                                         # å¾ž [-1, 1] normåˆ° [0, 1]
            ssim = calculate_ssim(predicted_val_img_full_norm, val_img_norm)
            
            # è¨˜éŒ„æŒ‡æ¨™åˆ° TensorBoard
            writer.add_scalar('val/MSE_metrics', mse, global_step=e)
            writer.add_scalar('val/MAE_metrics', mae, global_step=e)
            writer.add_scalar('val/PSNR_metrics', psnr, global_step=e)
            writer.add_scalar('val/SSIM_metrics', ssim, global_step=e)
            
            print(f'Validation Metrics - MSE: {mse:.6f}, MAE: {mae:.6f}, PSNR: {psnr:.2f} dB, SSIM: {ssim:.4f}')
            
            # å¦‚æžœç•¶å‰ SSIM æ˜¯æœ€ä½³çš„ï¼Œä¿å­˜ best æ¨¡åž‹
            if ssim > best_ssim:
                best_ssim = ssim
                best_model_path = args.model_path.replace('final.pt', 'best.pt')
                torch.save(model, best_model_path)
                print(f'ðŸŽ‰ New best model saved with SSIM: {ssim:.4f} at epoch {e}')
            


            # # ç´€éŒ„å¯è¦–åŒ–åœ–ç‰‡åˆ° tensorborad            
            # ä½¿ç”¨ F.interpolate å°‡åœ–ç‰‡æ”¾å¤§ 4x4 å€ï¼ˆå¾ž 32x32 åˆ° 128x128ï¼‰
            val_img_large = F.interpolate(val_img, scale_factor=4, mode='nearest')
            masked_img_large = F.interpolate(val_img * (1 - mask), scale_factor=4, mode='nearest')
            predicted_img_large = F.interpolate(predicted_val_img_full, scale_factor=4, mode='nearest')
            
            # # å°‡ batch å…§çš„æ‰€æœ‰çµæžœå¯è¦–åŒ–å¾Œæ‹šæˆå¤§åœ–
            # [16*3, c, h, w] # 16ç‚ºbatch size, 3æ˜¯æ¯å€‹batchä¸‰ç¨®åœ– (mask | pred | gt )
            img = torch.cat([masked_img_large, predicted_img_large, val_img_large], dim=0)

            # # å¤§åœ– å¯¬2*3å¼µå­åœ– é«˜8å¼µå­åœ–
            # æŠŠ 16(batch)*3(ç¨®åœ–) æ‹†æˆ (v=3, h1, w1=2) (å¯ä»¥ç®—å‡ºh1=8)
            # å¤§åœ–çš„é«˜=h1(æ•¸é‡)*h(å–®å€‹åœ–ç‰‡é«˜åº¦), å¯¬=w1(æ•¸é‡)*v(ä¸‰ç¨®åœ–)*w(å–®å€‹åœ–ç‰‡å¯¬åº¦)
            # img = rearrange(img, '(v h1 w1) c h w -> c (h1 h) (w1 v w)', w1=2, v=3) 

            # # å¤§åœ– å¯¬16å¼µå­åœ– é«˜3ç¨®åœ–
            # çµæžœæœƒæ˜¯ mask1 && mask2 && ... \\ pred1 && pred2 && ... \\ gt1 && gt2 && ...
            img = rearrange(img, '(v n) c h w -> c (v h) (n w)', v=3)  # 3è¡Œ16åˆ—
            writer.add_image('val/visualize', (img + 1) / 2, global_step=e)
        
        ''' save model '''
        torch.save(model, args.model_path)